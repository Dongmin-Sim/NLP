# 임베딩

> 한국어 임베딩 책을 참고하여 작성



자연어 처리 분야에서 임베딩은 **사람이 쓰는 언어**(자연어)를 **기계의 언어**(벡터)로 **바꾸는 결과** 혹은 **처리 과정**을 의미함. 단어나 문장을 vector로 변환해 embed 시킨다는 의미에서 임베딩이라고 불림. 



가장 기본적인 형태의 임베딩은 단어의 빈도(frequency)를 이용하여 임베딩을 하는 방법임. 예를 들어 다음과같은 문장이 있다고 했을때, 

> 문장1 : "Will you have time enough to do this for me tomorrow?"
>
> 문장2 : "How much will you need? I will need two thousand won."
>
> 문장3 : "Do you have time to buy something? How much is that?"

위의 문장들이 가지고 있는 단어들의 빈도를 이용하여 *단어-문서 행렬(Term-Document Matrix)*을 만들 수 있다.

| 구분     | 문장1 | 문장2 | 문장3 |
| -------- | ----- | ----- | ----- |
| tomorrow | 1     | 0     | 0     |
| Much     | 0     | 1     | 1     |
| Time     | 1     | 0     | 1     |

*단어-문서 행렬(Term-Document Matrix)* 을 통해 문장 1의 임베딩은 [1, 1, 1] 로 표현할 수 있고 much 라는 단어의 임베딩은 [0, 1, 1] 로 표현될 수 있음. 표를 보면 문장1과 문장 3에서 사용되는 단어가 문장 2에 비해 많이 겹치는 것을 확인할 수 있는데, 단어의 빈도를 통해 두 문장의 유사성을 대략적으로 파악할 수 있다. tomorrow 라는 단어는 문장 1에서만 등장한 것을 볼 수 있는데, 이는 상대적으로 `much-time` 두 단어의 의미 차이가 `tomorrow-much` 의 단어의 의미차이 보다 작다는 것을 유추할 수 있다. (`tomorrow-much` 의 의미차이가 `much-time` 보다 크다)



## 1. 임베딩의 역할

임베딩은 단어 문장간의 유사도를 계산하는 역할, 의미와 문법적 정보를 함축하는 역할, 전이학습을 해주는 역할을 한다.



### 1.1 단어/문장간 유사도(관련도) 계산

앞서 보았던 단어-문서 행렬은 매우 단순한 형태의 임베딩에 해당됨. 이보다  좀더 복잡한 임베딩 형태가 있는데, 구글에서 발표한 'Word2Vec' 기법이 대표적이다.  'Word2Vec' 기법은 단어들을 벡터로 바꿔주는데,  벡터로 변환함으로써 단어들 간의 유사도의 계산이 가능해짐.  워드 임베딩은 개별 단어를 밀집벡터로 표현하는 것을 말합니다. 밀집 벡터는 밀집표현방식으로 표현되는 벡터인데, 다음을 확인해보면 이해가 쉬움

#### 밀집표현(Dense Representation)

희소표현과 반대되는 표현입니다.**밀집표현(Dense Representation)**은 벡터의 차원을 단어사전의 크기로 보지 않습니다. 또한 사용자가 벡터의 차원을 설정해 줄 수 있습니다. 이 과정에서 0, 1만을 가진 벡터가 아닌 실수값을 가지는 벡터로 표현합니다. 즉, 모든 차원에 의미있는 정보를 가지게 되며, 0 대신 실수값들이 고르게 밀집되게 됩니다. 예시로 단어사전에 1000개의 단어가 있다고 가정할 때, 눈사람이라는 단어를 표현하기 위해선 다음과 같습니다.

```python
#희소표현
눈사람 = [0, 0, 1, 0, 0, 0, 0, 0, .....] # 벡터의 차원은 1000

#밀집표현
눈사람 = [0.2, 0.13, 0.5, -0.18, ....] # 벡터의 차원은 사용자가 지정한 차원의 수(e.g 120)
```

위와같이 밀집표현으로 표현되는 벡터들은 차원이 좁아졌기 때문에 밀집 벡터라고 합니다. 희소표현보다 벡터의 차원을 줄일 수 있음으로 희소벡터 보다는 밀집 벡터가 저차원이라고 할 수 있습니다. 위와 같이 단어를 밀집 벡터로 나타내는 것을 앞서 보았던 워드 임베딩이라고 하고, 벡터를 임베딩 벡터라고 합니다.

이러한 벡터의 특성을 이용하여 코사인 유사도로 단어들의 유사도를 계산이 가능해진다. 친환경으로 나아가는 발걸음으로 '전기차'에 대한 관심이 높아지고 있다. '전기차' 와 관련된 말뭉치를 통해 단어간 유사도를 word2vec 모델을 통해 확인해본다.





### 1.2 의미와 문법적 정보 함축

임베딩은 벡터이며, 벡터는 사칙 연산이 가능하다. 단어 벡터간 연산을 통해 단어들 사이의 의미, 문법적 관계를 도출할 수 있다. 



### 1.3 전이학습

임베딩은 다른 딥러닝 모델의 입력값으로 주로 사용됨. 이와 같이 다른 딥러닝 모델의 입력값으로 벡터를 사용하는 기법을 전이 학습이라고 한다. 매우 커다란 말뭉치를 사용하여 임베딩(의미적, 문법적 정보가 잘 함축되어 있는)을 미리 만들어 놓고, 이러한 임베딩을 입력값으로 쓰는 전이 학습 모델은 주어진 문제를 효율적으로 해결할 수 있다. 

