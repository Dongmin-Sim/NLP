# 임베딩

> 한국어 임베딩 책을 참고하여 작성



자연어 처리 분야에서 임베딩은 **사람이 쓰는 언어**(자연어)를 **기계의 언어**(벡터)로 **바꾸는 결과** 혹은 **처리 과정**을 의미함. 단어나 문장을 vector로 변환해 embed 시킨다는 의미에서 임베딩이라고 불림. 



가장 기본적인 형태의 임베딩은 단어의 빈도(frequency)를 이용하여 임베딩을 하는 방법임. 예를 들어 다음과같은 문장이 있다고 했을때, 

> 문장1 : "Will you have time enough to do this for me tomorrow?"
>
> 문장2 : "How much will you need? I will need two thousand won."
>
> 문장3 : "Do you have time to buy something? How much is that?"

위의 문장들이 가지고 있는 단어들의 빈도를 이용하여 *단어-문서 행렬(Term-Document Matrix)*을 만들 수 있다.

| 구분     | 문장1 | 문장2 | 문장3 |
| -------- | ----- | ----- | ----- |
| tomorrow | 1     | 0     | 0     |
| Much     | 0     | 1     | 1     |
| Time     | 1     | 0     | 1     |

*단어-문서 행렬(Term-Document Matrix)* 을 통해 문장 1의 임베딩은 [1, 1, 1] 로 표현할 수 있고 much 라는 단어의 임베딩은 [0, 1, 1] 로 표현될 수 있음. 표를 보면 문장1과 문장 3에서 사용되는 단어가 문장 2에 비해 많이 겹치는 것을 확인할 수 있는데, 단어의 빈도를 통해 두 문장의 유사성을 대략적으로 파악할 수 있다. tomorrow 라는 단어는 문장 1에서만 등장한 것을 볼 수 있는데, 이는 상대적으로 `much-time` 두 단어의 의미 차이가 `tomorrow-much` 의 단어의 의미차이 보다 작다는 것을 유추할 수 있다. (`tomorrow-much` 의 의미차이가 `much-time` 보다 크다)



## 1. 임베딩의 역할

임베딩은 단어 문장간의 유사도를 계산하는 역할, 의미와 문법적 정보를 함축하는 역할, 전이학습을 해주는 역할을 한다.



### 1.1 단어/문장간 유사도(관련도) 계산

앞서 보았던 단어-문서 행렬은 매우 단순한 형태의 임베딩에 해당됨. 이보다  좀더 복잡한 임베딩 형태가 있는데, 구글에서 발표한 'Word2Vec' 기법이 대표적이다.  'Word2Vec' 기법은 단어들을 벡터로 바꿔주는데,  벡터로 변환함으로써 단어들 간의 유사도의 계산이 가능해짐.  워드 임베딩은 개별 단어를 밀집벡터로 표현하는 것을 말합니다. 밀집 벡터는 밀집표현방식으로 표현되는 벡터인데, 다음을 확인해보면 이해가 쉬움

#### 밀집표현(Dense Representation)

희소표현과 반대되는 표현입니다.**밀집표현(Dense Representation)**은 벡터의 차원을 단어사전의 크기로 보지 않는다. 또한 사용자가 벡터의 차원을 설정해 줄 수 있다. 이 과정에서 0, 1만을 가진 벡터가 아닌 실수값을 가지는 벡터로 표현한다. 즉, 모든 차원에 의미있는 정보를 가지게 되며, 0 대신 실수값들이 고르게 밀집되게 된다. 예시로 단어사전에 1000개의 단어가 있다고 가정할 때, 눈사람이라는 단어를 표현하기 위해선 다음과 같다.

```python
#희소표현
눈사람 = [0, 0, 1, 0, 0, 0, 0, 0, .....] # 벡터의 차원은 1000

#밀집표현
눈사람 = [0.2, 0.13, 0.5, -0.18, ....] # 벡터의 차원은 사용자가 지정한 차원의 수(e.g 120)
```

위와같이 밀집표현으로 표현되는 벡터들은 차원이 좁아졌기 때문에 밀집 벡터라고 합니다. 희소표현보다 벡터의 차원을 줄일 수 있음으로 희소벡터 보다는 밀집 벡터가 저차원이라고 할 수 있다. 위와 같이 단어를 밀집 벡터로 나타내는 것을 앞서 보았던 워드 임베딩이라고 하고, 벡터를 임베딩 벡터라고 한다.

이러한 벡터의 특성을 이용하여 코사인 유사도로 단어들의 유사도를 계산이 가능해진다. 친환경으로 나아가는 발걸음으로 '전기차'에 대한 관심이 높아지고 있다. '전기차' 와 관련된 말뭉치를 통해 단어간 유사도를 word2vec 모델을 통해 확인해본다. [전기차 관련 뉴스 말뭉치를 통한 Word2Vec model 소스코드](https://github.com/Dongmin-Sim/NLP/blob/main/Embedding/word2vec_word_similarity.ipynb)

Naver 의 검색-뉴스 api 를 통해 키워드 '전기차' 에 대한 뉴스기사 1,000개에 대하여 kkma 형태소 분석기를 이용하여 기사 전처리 후 명사들만 추출하여 전기차 기사 말뭉치를 생성하였다. 

```python
# 전기차 기사 말뭉치(corpus)
[['신주', '포함', '지분', '확보', '스웨덴', '전기차', '기차', '폴스타', '스타', '천만', '달러', '투자', '그룹', '투자전문', '전문', '지주', '지주회사', '회사', '글로벌', '시장', '공략', '초급', '초급속', '충전기', '제조', '제조회사', '한국', '인수'], ['환경', '전기차', '기차', '인프라', '업체', '투자', '세계', '시장', '공략', '초급', '초급속', '충전기', '제조사', '넷이브', '이브', '인수', '유럽', '폴스타', '스타'], ['그룹', '투자', '투자전문회사', '전문', '회사', '국내', '초급', '초급속', '충전기', '제조사', '인수', '스웨덴', '전기차', '기차', '폴스타', '스타', '글로벌', '시장', '공략', '속도'], ['현대', '현대차그룹', '그룹', '전용', '전기차', '기차', '친환경차', '환경', '라인업', '확대', '중국', '중국시장', '시장', '도약', '사업', '전략', '전동', '올해', '출시', '예정', '아이오'], ['투자', '투자전문회사', '전문', '회사', '환경', '전기차', '기차', '인프라', '기술', '선점', '글로벌', '시장', '공략', '본격화', '선도', '초급', '초급속', '충전기', '제조', '제조회사', '한국', '인수', '전기', '충전', '본격', '진출', '한편']]
```

Word2Vec 모델 생성

```python
# Word2Vec model 생성
model = Word2Vec(docs,
                 size=100, 
                 window=4, 
                 min_count=2,
                 sg=1
                )
```

전기차 벡터 값 확인

```python
# 전기차의 벡터 값
model.wv['전기차']

array([ 0.12328565,  0.01489553,  0.0020914, ...  0.13057308, -0.13572884,  0.04051232, -0.16123982],
      dtype=float32)
```

전기차와 가장 유사한 단어 top10 개 추출 후 각 단어간 유사도 시각화

```python
전기차와 가장 유사한 단어 top10 :  ['충전', '초고속', '기차', '시범', '피트', '사진', '가운데', '오후', '확대', '초고']
```

<img width="667" alt="heatmap" src="https://user-images.githubusercontent.com/74139156/114917938-7bbe0880-9e61-11eb-9eda-51acc3f3135f.png">



t-SNE 기법으로 100 차원의 단어 벡터를 2차원으로 줄여 시각화해본 결과 다음과 같은 것을 확인할 수 있었음.

<img width="1734" alt="t-SNE" src="https://user-images.githubusercontent.com/74139156/114917752-431e2f00-9e61-11eb-987c-eb334e4b1af0.png">




### 1.2 의미와 문법적 정보 함축

임베딩은 벡터이며, 벡터는 사칙 연산이 가능하다. 단어 벡터간 연산을 통해 단어들 사이의 의미, 문법적 관계를 도출할 수 있다. 예를 들어 `아들` `딸` 사이의 관계와 `소년` `소녀` 의의미차이가 임베딩에 함축되어 있다면 좋은 임베딩이라고 할 수 있음. 



### 1.3 전이학습

임베딩은 다른 딥러닝 모델의 입력값으로 주로 사용됨. 이와 같이 다른 딥러닝 모델의 입력값으로 벡터를 사용하는 기법을 전이 학습이라고 한다. 매우 커다란 말뭉치를 사용하여 임베딩(의미적, 문법적 정보가 잘 함축되어 있는)을 미리 만들어 놓고, 이러한 임베딩을 입력값으로 쓰는 전이 학습 모델은 주어진 문제를 효율적으로 해결할 수 있다. 

품질이 좋은 임베딩을 사용하는 것은 모델에서의 정확도와 학습속도를 높일 수 있다. 



## 2. 임베딩 기법의 역사 및 종류

### 2.1 통계기반 -> 뉴럴 네트워크 기반으로의 변화

초기의 임베딩 기법은 말뭉치의 통계량을 직접적으로 활용하는 경우가 많았다. 대표적인 기법이 잠재의미분석(Latent Semantic Analysis, LSA))기법인데 SVD 를 활용하여 문서에 함축된 주제를 찾아내는 기법이다. [SVD 특이값 분해 참고 자료](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-20-%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4Singular-Value-Decomposition), 잠재 의미 분석 수행 대상 행렬은 여러 종류가 될 수 있는데, TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보량 행렬 등이 있다. 

 최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 많이 주목받고 있으며, 구조가 유연하고 표현력이 풍부하여 자연어의 문맥의 대부분을 학습할 수 있다는 장점이 있다. 



### 2.2 단어수준 임베딩과 문장수준 임베딩

2017년 이전의 임베딩 기법들은 주로 단어 수준의 모델이 주를 이뤘음. 그러나 2018 년 초 ELMo 모델이 발표된 이후 문장 수준의 임베딩 기법들이 주목을 받기 시작했음. 

#### 2.2.1 문장 임베딩

- 문장 전체의 흐름을 파악해 벡터로 표현합니다.

- 문맥적 의미를 지니기 때문에 품질이 좋아 상용시스템으로 사용됩니다.

- but, 많은 문장과 학습 비용이 높습니다.

  - 예시)

    ELMo(Embeddings from Language Models)

    BERT(Bidirectional Encoder Representations from Transformer)

    GPT(Generation Pre-Training) 등

#### 2.2.2 단어 임베딩

- 개별 단어를 벡터로 표현합니다.

- 학습방법이 간단해 실무에서 많이 사용됩니다.

- but, 문맥의 의미가 없어 동음이의어의 의미차이를 반영하지 않고 동일한 벡터값으로 표현합니다. (e.g 먹는배, 타는배, 배꼽배 를 모두 같은 벡터값으로 표현)

  - 예시)

    NPLM

    Word2Vec

    GloVe

    FastText

    Swivel
    
    

### 2.3 룰 -> end-to-end -> pertained/fine-tuning

1990 년대까지 자연어처리 모델은 거의 대부분이 사람이 가진 전통적인 도메인 지식(언어학적인 지식)을 이용해 모델의 피처를 정의했었다.

2000 년대 중반 이후 부터는 딥러닝 모델이 주목 받기 시작하면서 데이터를 통째로 모델에 넣고 모델 스스로 피처를 뽑아내는 `end-to-end` 기법이 주를 이뤘다.

ELMo 모델이 제안(2018)된 이후, 자연어 처리 모델은 `end-to-end` 방식에서 벗어나 `pretrained/fine-tuning` 방식으로 변화하고 있음. 대규모의 말뭉치로 임베딩을 생성한 후, 해결해야 하는 task 에 따라 임베딩을 입력으로 하는 딥러닝 모델을 만들고, 이를 새로운 데이터에 맞게 모델 전체를 튜닝 및 전이학습을 시키는 것을 의미한다. `ELMo`, `BERT`,` GPT` 방식이 이에 해당됨. 

자연어와 관련된 task 는 다음과 같이 나누어질 수 있다. 

**다운스트림 태스크 (downstream task)** : 구체화된 자연어 문제들을 의미하며 다음과 같은 문제들이 이에 해당됨. `품사판별`, `개체명 인식`, `의존관계분석`,`의미역분석`, `상호 참조 해결` 등이 있다.

**업스트림 태스크 (upstream task)** : 다운스트임 태스크를 해결하기 위해 전제되어야 하는 과제를 의미함. 임베딩을 pretrain 하는 과정이 이에 해당될 수 있다. 



### 2.4 임베딩의 종류와 성능

임베딩 기법은 방식에 따라 다음과 같이 종류가 나누어질 수 있다.

* 행렬 분해 기반
  * 맒우치 정보가 있는 행렬을 작은 행렬로 쪼개는 방식의 임베딩 기법을 의미하며 Glove, Swivel 등이 이에 해당됨. 
* 에측기반
  * 단어 주변에 특정 단어가 등장하는지 예측하거나, 다음 단어가 무엇인지 예측하거나, 문장내 일부 단어를 지우고 맞추는 과정에서 학습하는 방법임. 보통 뉴럴 네트워크 방법들이 예측기반에 해당된다고 보면 됨. Word2Vec, FastText, BERT, ELMo, GPT 등이 있음.
* 토픽기반
  * 주어진 문서에 대해 주제를 추론하는 방식으로 임베딩을 하는 기법임. LDA 가 대표적인 기법임. 이 모델은 학습이 완료되면 문서가 어떤 주제 분포를 갖는지 벡터로 변환함. 