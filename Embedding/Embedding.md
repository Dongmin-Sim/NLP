# 임베딩

> 한국어 임베딩 책을 참고하여 작성



자연어 처리 분야에서 임베딩은 **사람이 쓰는 언어**(자연어)를 **기계의 언어**(벡터)로 **바꾸는 결과** 혹은 **처리 과정**을 의미함. 단어나 문장을 vector로 변환해 embed 시킨다는 의미에서 임베딩이라고 불림. 



가장 기본적인 형태의 임베딩은 단어의 빈도(frequency)를 이용하여 임베딩을 하는 방법임. 예를 들어 다음과같은 문장이 있다고 했을때, 

> 문장1 : "Will you have time enough to do this for me tomorrow?"
>
> 문장2 : "How much will you need? I will need two thousand won."
>
> 문장3 : "Do you have time to buy something? How much is that?"

위의 문장들이 가지고 있는 단어들의 빈도를 이용하여 *단어-문서 행렬(Term-Document Matrix)*을 만들 수 있다.

| 구분     | 문장1 | 문장2 | 문장3 |
| -------- | ----- | ----- | ----- |
| tomorrow | 1     | 0     | 0     |
| Much     | 0     | 1     | 1     |
| Time     | 1     | 0     | 1     |

*단어-문서 행렬(Term-Document Matrix)* 을 통해 문장 1의 임베딩은 [1, 1, 1] 로 표현할 수 있고 much 라는 단어의 임베딩은 [0, 1, 1] 로 표현될 수 있음. 표를 보면 문장1과 문장 3에서 사용되는 단어가 문장 2에 비해 많이 겹치는 것을 확인할 수 있는데, 단어의 빈도를 통해 두 문장의 유사성을 대략적으로 파악할 수 있다. tomorrow 라는 단어는 문장 1에서만 등장한 것을 볼 수 있는데, 이는 상대적으로 `much-time` 두 단어의 의미 차이가 `tomorrow-much` 의 단어의 의미차이 보다 작다는 것을 유추할 수 있다. (`tomorrow-much` 의 의미차이가 `much-time` 보다 크다)



## 1. 임베딩의 역할

임베딩은 단어 문장간의 유사도를 계산하는 역할, 의미와 문법적 정보를 함축하는 역할, 전이학습을 해주는 역할을 한다.



### 1.1 단어/문장간 유사도(관련도) 계산

앞서 보았던 단어-문서 행렬은 매우 단순한 형태의 임베딩에 해당됨. 이보다  좀더 복잡한 임베딩 형태가 있는데, 구글에서 발표한 'Word2Vec' 기법이 대표적이다.  'Word2Vec' 기법은 단어들을 벡터로 바꿔주는데,  벡터로 변환함으로써 단어들 간의 유사도의 계산이 가능해짐.  워드 임베딩은 개별 단어를 밀집벡터로 표현하는 것을 말합니다. 밀집 벡터는 밀집표현방식으로 표현되는 벡터인데, 다음을 확인해보면 이해가 쉬움

#### 밀집표현(Dense Representation)

희소표현과 반대되는 표현입니다.**밀집표현(Dense Representation)**은 벡터의 차원을 단어사전의 크기로 보지 않습니다. 또한 사용자가 벡터의 차원을 설정해 줄 수 있습니다. 이 과정에서 0, 1만을 가진 벡터가 아닌 실수값을 가지는 벡터로 표현합니다. 즉, 모든 차원에 의미있는 정보를 가지게 되며, 0 대신 실수값들이 고르게 밀집되게 됩니다. 예시로 단어사전에 1000개의 단어가 있다고 가정할 때, 눈사람이라는 단어를 표현하기 위해선 다음과 같습니다.

```python
#희소표현
눈사람 = [0, 0, 1, 0, 0, 0, 0, 0, .....] # 벡터의 차원은 1000

#밀집표현
눈사람 = [0.2, 0.13, 0.5, -0.18, ....] # 벡터의 차원은 사용자가 지정한 차원의 수(e.g 120)
```

위와같이 밀집표현으로 표현되는 벡터들은 차원이 좁아졌기 때문에 밀집 벡터라고 합니다. 희소표현보다 벡터의 차원을 줄일 수 있음으로 희소벡터 보다는 밀집 벡터가 저차원이라고 할 수 있습니다. 위와 같이 단어를 밀집 벡터로 나타내는 것을 앞서 보았던 워드 임베딩이라고 하고, 벡터를 임베딩 벡터라고 합니다.

이러한 벡터의 특성을 이용하여 코사인 유사도로 단어들의 유사도를 계산이 가능해진다. 친환경으로 나아가는 발걸음으로 '전기차'에 대한 관심이 높아지고 있다. '전기차' 와 관련된 말뭉치를 통해 단어간 유사도를 word2vec 모델을 통해 확인해본다. [전기차 관련 뉴스 말뭉치를 통한 Word2Vec model 소스코드](https://github.com/Dongmin-Sim/NLP/blob/main/Embedding/word2vec_word_similarity.ipynb)

Naver 의 검색-뉴스 api 를 통해 키워드 '전기차' 에 대한 뉴스기사 1,000개에 대하여 kkma 형태소 분석기를 이용하여 기사 전처리 후 명사들만 추출하여 전기차 기사 말뭉치를 생성하였다. 

```python
# 전기차 기사 말뭉치(corpus)
[['신주', '포함', '지분', '확보', '스웨덴', '전기차', '기차', '폴스타', '스타', '천만', '달러', '투자', '그룹', '투자전문', '전문', '지주', '지주회사', '회사', '글로벌', '시장', '공략', '초급', '초급속', '충전기', '제조', '제조회사', '한국', '인수'], ['환경', '전기차', '기차', '인프라', '업체', '투자', '세계', '시장', '공략', '초급', '초급속', '충전기', '제조사', '넷이브', '이브', '인수', '유럽', '폴스타', '스타'], ['그룹', '투자', '투자전문회사', '전문', '회사', '국내', '초급', '초급속', '충전기', '제조사', '인수', '스웨덴', '전기차', '기차', '폴스타', '스타', '글로벌', '시장', '공략', '속도'], ['현대', '현대차그룹', '그룹', '전용', '전기차', '기차', '친환경차', '환경', '라인업', '확대', '중국', '중국시장', '시장', '도약', '사업', '전략', '전동', '올해', '출시', '예정', '아이오'], ['투자', '투자전문회사', '전문', '회사', '환경', '전기차', '기차', '인프라', '기술', '선점', '글로벌', '시장', '공략', '본격화', '선도', '초급', '초급속', '충전기', '제조', '제조회사', '한국', '인수', '전기', '충전', '본격', '진출', '한편']]
```

Word2Vec 모델 생성

```python
# Word2Vec model 생성
model = Word2Vec(docs,
                 size=100, 
                 window=4, 
                 min_count=2,
                 sg=1
                )
```

전기차 벡터 값 확인

```python
# 전기차의 벡터 값
model.wv['전기차']

array([ 0.12328565,  0.01489553,  0.0020914, ...  0.13057308, -0.13572884,  0.04051232, -0.16123982],
      dtype=float32)
```

전기차와 가장 유사한 단어 top10 개 추출 후 각 단어간 유사도 시각화

```python
전기차와 가장 유사한 단어 top10 :  ['충전', '초고속', '기차', '시범', '피트', '사진', '가운데', '오후', '확대', '초고']
```

<img width="667" alt="heatmap" src="https://user-images.githubusercontent.com/74139156/114917938-7bbe0880-9e61-11eb-9eda-51acc3f3135f.png">



t-SNE 기법으로 100 차원의 단어 벡터를 2차원으로 줄여 시각화해본 결과 다음과 같은 것을 확인할 수 있었음.

<img width="1734" alt="t-SNE" src="https://user-images.githubusercontent.com/74139156/114917752-431e2f00-9e61-11eb-987c-eb334e4b1af0.png">




### 1.2 의미와 문법적 정보 함축

임베딩은 벡터이며, 벡터는 사칙 연산이 가능하다. 단어 벡터간 연산을 통해 단어들 사이의 의미, 문법적 관계를 도출할 수 있다. 



### 1.3 전이학습

임베딩은 다른 딥러닝 모델의 입력값으로 주로 사용됨. 이와 같이 다른 딥러닝 모델의 입력값으로 벡터를 사용하는 기법을 전이 학습이라고 한다. 매우 커다란 말뭉치를 사용하여 임베딩(의미적, 문법적 정보가 잘 함축되어 있는)을 미리 만들어 놓고, 이러한 임베딩을 입력값으로 쓰는 전이 학습 모델은 주어진 문제를 효율적으로 해결할 수 있다. 





## 2. 임베딩 기법의 역사 및 종류



### 2.1 통계기반 -> 뉴럴 네트워크 기반으로의 변화





### 2.2 단어수준 임베딩과 문장수준 임베딩

2017년 이전의 임베딩 기법들은 주로 단어 수준의 모델이 주를 이뤘음. 그러나 2018 년 초 ELMo 모델이 발표된 이후 문장 수준의 임베딩 기법들이 주목을 받기 시작했음. 

#### 2.2.1 문장 임베딩

- 문장 전체의 흐름을 파악해 벡터로 표현합니다.

- 문맥적 의미를 지니기 때문에 품질이 좋아 상용시스템으로 사용됩니다.

- but, 많은 문장과 학습 비용이 높습니다.

  - 예시)

    ELMo(Embeddings from Language Models)

    BERT(Bidirectional Encoder Representations from Transformer)

    GPT(Generation Pre-Training) 등

#### 2.2.2 단어 임베딩

- 개별 단어를 벡터로 표현합니다.

- 학습방법이 간단해 실무에서 많이 사용됩니다.

- but, 문맥의 의미가 없어 동음이의어의 의미차이를 반영하지 않고 동일한 벡터값으로 표현합니다. (e.g 먹는배, 타는배, 배꼽배 를 모두 같은 벡터값으로 표현)

  - 예시)

    NPLM

    Word2Vec

    GloVe

    FastText

    Swivel



### 2.3 임베딩의 종류
